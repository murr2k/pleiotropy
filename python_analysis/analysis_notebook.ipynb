{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic Pleiotropy Analysis Notebook\n",
    "\n",
    "This interactive notebook provides tools for exploring and analyzing genomic pleiotropy data using the Python analysis modules and Rust core functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Import our custom modules\n",
    "from trait_visualizer import TraitVisualizer\n",
    "from statistical_analyzer import StatisticalAnalyzer\n",
    "from rust_interface import RustInterface, InterfaceMode, dataframe_to_trait_data, results_to_dataframe\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analysis components\n",
    "visualizer = TraitVisualizer(style='seaborn-v0_8')\n",
    "analyzer = StatisticalAnalyzer(multiple_testing_method='fdr_bh')\n",
    "\n",
    "# Initialize Rust interface (subprocess mode by default)\n",
    "# Change to InterfaceMode.PYO3 if you have built the PyO3 extension\n",
    "rust_interface = RustInterface(mode=InterfaceMode.SUBPROCESS)\n",
    "\n",
    "print(\"Components initialized!\")\n",
    "print(f\"Rust core version: {rust_interface.get_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Decrypt Trait Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load encrypted trait data\n",
    "# Replace with your actual file paths\n",
    "encrypted_file = \"../data/encrypted_traits.dat\"\n",
    "key_file = \"../data/trait_key.key\"\n",
    "\n",
    "# Decrypt and load trait data\n",
    "try:\n",
    "    trait_df = rust_interface.decrypt_trait_data(\n",
    "        encrypted_file=encrypted_file,\n",
    "        key_file=key_file,\n",
    "        output_format='dataframe'\n",
    "    )\n",
    "    print(f\"Loaded {trait_df.shape[0]} samples with {trait_df.shape[1]} traits\")\n",
    "    print(\"\\nTrait names:\")\n",
    "    print(list(trait_df.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading encrypted data: {e}\")\n",
    "    print(\"Generating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Generate synthetic trait data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_traits = 10\n",
    "    \n",
    "    # Create correlated traits\n",
    "    trait_names = [f\"Trait_{i+1}\" for i in range(n_traits)]\n",
    "    \n",
    "    # Generate correlation structure\n",
    "    base_data = np.random.randn(n_samples, n_traits)\n",
    "    \n",
    "    # Add correlations between some traits\n",
    "    base_data[:, 1] = base_data[:, 0] * 0.7 + np.random.randn(n_samples) * 0.3\n",
    "    base_data[:, 2] = base_data[:, 0] * 0.5 + base_data[:, 1] * 0.3 + np.random.randn(n_samples) * 0.2\n",
    "    base_data[:, 4] = base_data[:, 3] * 0.8 + np.random.randn(n_samples) * 0.2\n",
    "    \n",
    "    trait_df = pd.DataFrame(base_data, columns=trait_names)\n",
    "    print(f\"Generated synthetic data: {trait_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "trait_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trait Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trait correlations with p-values\n",
    "corr_matrix, p_matrix = analyzer.calculate_trait_correlations(\n",
    "    trait_df, \n",
    "    method='pearson'\n",
    ")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig = visualizer.plot_trait_correlation_heatmap(\n",
    "    trait_df,\n",
    "    method='pearson',\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Find significant correlations\n",
    "significant_corrs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if p_matrix.iloc[i, j] < 0.05:\n",
    "            significant_corrs.append({\n",
    "                'Trait1': corr_matrix.columns[i],\n",
    "                'Trait2': corr_matrix.columns[j],\n",
    "                'Correlation': corr_matrix.iloc[i, j],\n",
    "                'P-value': p_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "sig_corr_df = pd.DataFrame(significant_corrs).sort_values('P-value')\n",
    "print(f\"\\nFound {len(sig_corr_df)} significant correlations (p < 0.05):\")\n",
    "sig_corr_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive heatmap\n",
    "interactive_fig = visualizer.create_interactive_heatmap(trait_df, method='pearson')\n",
    "interactive_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trait Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of each trait\n",
    "n_cols = 3\n",
    "n_rows = (len(trait_df.columns) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, trait in enumerate(trait_df.columns):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        trait_df[trait].hist(bins=30, ax=ax, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_title(f'Distribution of {trait}')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = trait_df[trait].mean()\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "        ax.legend()\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(len(trait_df.columns), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca_results = analyzer.perform_pca_analysis(trait_df, n_components=5, standardize=True)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax1.plot(range(1, len(pca_results['explained_variance_ratio']) + 1), \n",
    "         pca_results['explained_variance_ratio'], \n",
    "         'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance explained\n",
    "ax2.plot(range(1, len(pca_results['cumulative_variance_ratio']) + 1),\n",
    "         pca_results['cumulative_variance_ratio'],\n",
    "         'ro-', linewidth=2, markersize=8)\n",
    "ax2.axhline(0.8, color='green', linestyle='--', label='80% variance')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.set_title('Cumulative Variance Explained')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance explained by first 3 components: {pca_results['cumulative_variance_ratio'][2]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA biplot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot samples\n",
    "pc_data = pca_results['transformed_data']\n",
    "ax.scatter(pc_data['PC1'], pc_data['PC2'], alpha=0.5, s=30)\n",
    "\n",
    "# Plot loadings as arrows\n",
    "loadings = pca_results['loadings']\n",
    "for trait in loadings.index:\n",
    "    ax.arrow(0, 0, \n",
    "             loadings.loc[trait, 'PC1'] * 3, \n",
    "             loadings.loc[trait, 'PC2'] * 3,\n",
    "             head_width=0.05, head_length=0.05, \n",
    "             fc='red', ec='red', alpha=0.7)\n",
    "    ax.text(loadings.loc[trait, 'PC1'] * 3.2, \n",
    "            loadings.loc[trait, 'PC2'] * 3.2,\n",
    "            trait, fontsize=10, ha='center', va='center')\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({pca_results['explained_variance_ratio'][0]:.1%} variance)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca_results['explained_variance_ratio'][1]:.1%} variance)\")\n",
    "ax.set_title('PCA Biplot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trait Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster traits based on their patterns\n",
    "clustering_results = analyzer.cluster_traits(trait_df, method='kmeans', n_clusters=3)\n",
    "\n",
    "# Display cluster assignments\n",
    "cluster_df = pd.DataFrame({\n",
    "    'Trait': list(clustering_results['trait_clusters'].keys()),\n",
    "    'Cluster': list(clustering_results['trait_clusters'].values())\n",
    "}).sort_values('Cluster')\n",
    "\n",
    "print(f\"Traits grouped into {clustering_results['n_clusters']} clusters:\")\n",
    "for cluster_id in range(clustering_results['n_clusters']):\n",
    "    traits_in_cluster = cluster_df[cluster_df['Cluster'] == cluster_id]['Trait'].tolist()\n",
    "    print(f\"\\nCluster {cluster_id}: {', '.join(traits_in_cluster)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gene-Trait Association Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic gene-trait associations for demonstration\n",
    "# In real analysis, this would come from GWAS or other association studies\n",
    "np.random.seed(42)\n",
    "\n",
    "genes = [f\"GENE{i}\" for i in range(1, 21)]\n",
    "gene_trait_associations = {}\n",
    "\n",
    "for gene in genes:\n",
    "    # Randomly assign traits to genes\n",
    "    n_traits = np.random.choice([1, 2, 3, 4, 5], p=[0.3, 0.3, 0.2, 0.15, 0.05])\n",
    "    associated_traits = np.random.choice(trait_df.columns, size=n_traits, replace=False).tolist()\n",
    "    gene_trait_associations[gene] = associated_traits\n",
    "\n",
    "# Calculate pleiotropy scores\n",
    "pleiotropy_scores = analyzer.calculate_pleiotropy_score(\n",
    "    gene_trait_associations,\n",
    "    trait_correlations=corr_matrix,\n",
    "    method='count_weighted'\n",
    ")\n",
    "\n",
    "# Visualize pleiotropy scores\n",
    "fig = visualizer.plot_pleiotropy_score_distribution(\n",
    "    pleiotropy_scores,\n",
    "    threshold=2.5\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gene-Trait Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top pleiotropic genes for visualization\n",
    "sorted_genes = sorted(pleiotropy_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_genes = dict(sorted_genes[:10])\n",
    "top_gene_associations = {gene: gene_trait_associations[gene] for gene in top_genes.keys()}\n",
    "\n",
    "# Create network visualization\n",
    "fig = visualizer.plot_gene_trait_network(\n",
    "    top_gene_associations,\n",
    "    layout='spring',\n",
    "    node_size_factor=500\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Sankey Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sankey diagram for gene-trait relationships\n",
    "sankey_fig = visualizer.create_trait_gene_sankey(\n",
    "    gene_trait_associations,\n",
    "    top_n_genes=15\n",
    ")\n",
    "sankey_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for export\n",
    "results_dir = Path(\"./analysis_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export correlation matrix\n",
    "corr_matrix.to_csv(results_dir / \"trait_correlations.csv\")\n",
    "print(\"Saved trait correlations\")\n",
    "\n",
    "# Export pleiotropy scores\n",
    "pleiotropy_df = pd.DataFrame([\n",
    "    {'gene': gene, 'pleiotropy_score': score, 'n_traits': len(gene_trait_associations[gene])}\n",
    "    for gene, score in pleiotropy_scores.items()\n",
    "]).sort_values('pleiotropy_score', ascending=False)\n",
    "\n",
    "pleiotropy_df.to_csv(results_dir / \"pleiotropy_scores.csv\", index=False)\n",
    "print(\"Saved pleiotropy scores\")\n",
    "\n",
    "# Export PCA results\n",
    "pca_results['loadings'].to_csv(results_dir / \"pca_loadings.csv\")\n",
    "pca_results['transformed_data'].to_csv(results_dir / \"pca_scores.csv\")\n",
    "print(\"Saved PCA results\")\n",
    "\n",
    "# Create summary report\n",
    "summary = {\n",
    "    'n_samples': len(trait_df),\n",
    "    'n_traits': len(trait_df.columns),\n",
    "    'n_genes_analyzed': len(gene_trait_associations),\n",
    "    'n_significant_correlations': len(sig_corr_df),\n",
    "    'max_pleiotropy_score': max(pleiotropy_scores.values()),\n",
    "    'variance_explained_3pc': float(pca_results['cumulative_variance_ratio'][2])\n",
    "}\n",
    "\n",
    "with open(results_dir / \"analysis_summary.json\", 'w') as f:\n",
    "    import json\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nAnalysis complete! Results saved to:\", results_dir.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Custom Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom analysis functions for your specific needs\n",
    "\n",
    "def find_trait_pairs_with_high_correlation(corr_matrix, threshold=0.7):\n",
    "    \"\"\"Find pairs of traits with correlation above threshold.\"\"\"\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr = corr_matrix.iloc[i, j]\n",
    "            if abs(corr) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'trait1': corr_matrix.columns[i],\n",
    "                    'trait2': corr_matrix.columns[j],\n",
    "                    'correlation': corr\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(high_corr_pairs)\n",
    "\n",
    "def identify_hub_genes(gene_trait_associations, min_traits=3):\n",
    "    \"\"\"Identify hub genes associated with many traits.\"\"\"\n",
    "    hub_genes = {\n",
    "        gene: traits \n",
    "        for gene, traits in gene_trait_associations.items() \n",
    "        if len(traits) >= min_traits\n",
    "    }\n",
    "    return hub_genes\n",
    "\n",
    "# Example usage\n",
    "high_corr_pairs = find_trait_pairs_with_high_correlation(corr_matrix, threshold=0.5)\n",
    "print(f\"Found {len(high_corr_pairs)} trait pairs with |correlation| > 0.5\")\n",
    "high_corr_pairs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hub genes\n",
    "hub_genes = identify_hub_genes(gene_trait_associations, min_traits=3)\n",
    "print(f\"Found {len(hub_genes)} hub genes associated with 3+ traits:\")\n",
    "for gene, traits in sorted(hub_genes.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"  {gene}: {len(traits)} traits - {', '.join(traits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provides a foundation for genomic pleiotropy analysis. You can extend it by:\n",
    "\n",
    "1. **Loading real genomic data**: Replace synthetic data with actual GWAS results\n",
    "2. **Adding pathway analysis**: Integrate pathway databases to understand biological functions\n",
    "3. **Implementing advanced statistics**: Add more sophisticated statistical tests\n",
    "4. **Creating custom visualizations**: Develop specialized plots for your specific research questions\n",
    "5. **Building machine learning models**: Use the processed data for predictive modeling\n",
    "\n",
    "Remember to save your work and document your findings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}